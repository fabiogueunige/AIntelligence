{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the environment model (simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract interface for any environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnvironment(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to return the set of states\n",
    "    def get_states(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to return the set of actions available in the state\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_all_actions(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement p(next_state | current_state, action)\n",
    "    def p(self, current_state, action, next_state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to implement r(current_state, action, next_state)\n",
    "    def r(self, current_state, action, next_state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>GridWorld</code> environment: a 4x4 grid where the agent can move up-down and left-right (no diagonal moves). Transitions are deterministic. Cell (3,3) is the target and it is the only one with a chance to stop. Rewards are -1 for every move and 10 when the target is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnvironment):\n",
    "    \n",
    "    UP = (-1,0)\n",
    "    DOWN = (1,0)\n",
    "    LEFT = (0,-1)\n",
    "    RIGHT = (0,1)\n",
    "    STAY = (0,0)\n",
    "    \n",
    "    label = {(-1,0):\"UP\", (1,0):\"DOWN\", (0,-1):\"LEFT\", (0,1):\"RIGHT\", (0,0):\"STAY\"}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = list()\n",
    "        for ri in range(4):\n",
    "            for co in range(4):\n",
    "                self.states.append((ri,co))\n",
    "        self.actions = dict()\n",
    "        self.actions[(0,0)] = [GridWorld.RIGHT, GridWorld.DOWN]\n",
    "        self.actions[(0,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,3)] = [GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(1,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,3)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,3)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,0)] = [GridWorld.RIGHT, GridWorld.UP]\n",
    "        self.actions[(3,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(3,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(3,3)] = [GridWorld.STAY, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.all_actions = [GridWorld.STAY, GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "        \n",
    "    def get_actions(self, state):\n",
    "        return self.actions[state]\n",
    "    \n",
    "    def get_all_actions(self, state):\n",
    "        return all_actions\n",
    "    \n",
    "    def transition(self, current_state, action):\n",
    "        return (current_state[0] + action[0], current_state[1] + action[1]) \n",
    "    \n",
    "    def p(self, current_state, action, next_state):\n",
    "        # If the action is not executable in the state, whatever the next state is, the probability is 0\n",
    "        if action not in self.get_actions(current_state):\n",
    "            return 0\n",
    "        # Compute the new state based on the action\n",
    "        new_state = self.transition(current_state, action)\n",
    "        # If the new state coincides with the next_state the probability is 1 \n",
    "        # otherwise it is 0 (deterministic transitions)\n",
    "        if new_state == next_state:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def r(self, current_state, action, next_state):\n",
    "        # If it is possible to reach the next state from the current one...\n",
    "        if self.p(current_state, action, next_state) == 1:\n",
    "            if (next_state != (3,3)):\n",
    "                # ...the reward is -1 for every next state...\n",
    "                return -1\n",
    "            else:\n",
    "                # ...except the goal \n",
    "                return 10\n",
    "        # ... otherwise the reward is zero (just for correctness, but the robot will not go there)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base class for all policies. The method <code>apply</code> enforces the policy, the method <code>update</code> changes the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement policy application\n",
    "    # Returns the action given the state\n",
    "    def apply(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement policy update\n",
    "    # Sets the action to be returned for the given state\n",
    "    def update(self, state, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concrete policy to initialize policy-search algorithms. Initially, some action among those available for the state is chosen uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.state_action_table = dict()\n",
    "        for state in environment.get_states():\n",
    "            # Initially, no action is specified\n",
    "            self.state_action_table[state] = None\n",
    "        \n",
    "    def apply(self, state):\n",
    "        if self.state_action_table[state] == None:\n",
    "            # When no action is specified, choose uniformly at random among available actions\n",
    "            actions = self.environment.get_actions(state)\n",
    "            return random.choice(actions)\n",
    "        # If an action is available for the state, return that action\n",
    "        return self.state_action_table[state]\n",
    "    \n",
    "    def update(self, state, action):\n",
    "        self.state_action_table[state] = action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \n",
    "    def __init__(self, environment, gamma, theta):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.policy = CustomPolicy(environment)\n",
    "        # The value of states at step k\n",
    "        self.old_value = dict()\n",
    "        # The value of states at step k+1\n",
    "        self.new_value = dict()\n",
    "        # Initialize the value of each state to 0\n",
    "        for state in environment.get_states():\n",
    "            self.old_value[state] = 0\n",
    "            \n",
    "    def copy_values(self, from_value, to_value):\n",
    "        for state in self.environment.get_states():\n",
    "            to_value[state] = from_value[state]\n",
    "            \n",
    "    def policy_evaluation(self):\n",
    "        while (True):\n",
    "            delta = 0\n",
    "            for state in self.environment.get_states():\n",
    "                # Compute the new value of the state\n",
    "                action = self.policy.apply(state)\n",
    "                self.new_value[state] = 0\n",
    "                for next_state in self.environment.get_states():\n",
    "                    self.new_value[state] += self.environment.p(state, action, next_state)*\\\n",
    "                    (self.environment.r(state, action, next_state) + self.gamma * self.old_value[next_state])\n",
    "                delta = max(delta, abs(self.old_value[state] - self.new_value[state]))\n",
    "            # Copy the new value of states into the old ones\n",
    "            self.copy_values(self.new_value, self.old_value)\n",
    "            if delta < self.theta:\n",
    "                return\n",
    "            \n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for state in self.environment.get_states():\n",
    "            old_action = self.policy.apply(state)\n",
    "            # Compute the action yielding the maximum return\n",
    "            max_value_of_action = -1 *  sys.float_info.max\n",
    "            best_action = old_action\n",
    "            for action in self.environment.get_actions(state):\n",
    "                value_of_action = 0\n",
    "                for next_state in self.environment.get_states():\n",
    "                    value_of_action += self.environment.p(state, action, next_state) *\\\n",
    "                    (self.environment.r(state, action, next_state) + self.gamma * self.old_value[next_state])\n",
    "                if value_of_action > max_value_of_action:\n",
    "                    best_action = action\n",
    "                    max_value_of_action = value_of_action\n",
    "            if old_action != best_action:\n",
    "                self.policy.update(state, best_action)\n",
    "                policy_stable = False\n",
    "        return policy_stable\n",
    "    \n",
    "    def apply(self):\n",
    "        while (True):\n",
    "            self.policy_evaluation()\n",
    "            if self.policy_improvement():\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "policy_iteration = PolicyIteration(grid_world, 0.8, 1/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iteration.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the optimal state values (optimal value function $v^*$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.022400000000003 17.528000000000002 23.160000000000004 30.200000000000003 \n",
      "17.528000000000002 23.160000000000004 30.200000000000003 39.0 \n",
      "23.160000000000004 30.200000000000003 39.0 50.0 \n",
      "30.200000000000003 39.0 50.0 50.0 \n"
     ]
    }
   ],
   "source": [
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(policy_iteration.old_value[(ri,co)],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the optimal policy $\\pi^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT STAY \n"
     ]
    }
   ],
   "source": [
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[policy_iteration.policy.apply((ri,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, let us check what happens with a single step of policy evaluation with a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iteration = PolicyIteration(grid_world, 0.8, 1/100000)\n",
    "policy_iteration.policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.999999999999998 -4.999999908292665 -4.999999999999998 -4.999999908292665 \n",
      "-4.999999908292665 -4.999999999999998 -4.999999908292665 -4.999999999999998 \n",
      "-4.999999999999998 -4.999999908292665 -4.999999999999998 -4.999999908292665 \n",
      "-4.999999908292665 -4.999999999999998 -4.999999908292665 -4.999999999999998 \n"
     ]
    }
   ],
   "source": [
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(policy_iteration.old_value[(ri,co)],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check a single policy improvement step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT LEFT DOWN LEFT \n",
      "RIGHT UP RIGHT UP \n",
      "UP RIGHT UP DOWN \n",
      "UP RIGHT RIGHT STAY \n"
     ]
    }
   ],
   "source": [
    "result = policy_iteration.policy_improvement()\n",
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[policy_iteration.policy.apply((ri,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As exxpected, the result is <code>False</code> since the initial policy was improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see another step to show the beginnings of convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWN DOWN DOWN DOWN \n",
      "DOWN DOWN UP LEFT \n",
      "DOWN UP DOWN DOWN \n",
      "RIGHT RIGHT RIGHT STAY \n",
      "False\n"
     ]
    }
   ],
   "source": [
    "policy_iteration.policy_evaluation()\n",
    "result = policy_iteration.policy_improvement()\n",
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[policy_iteration.policy.apply((ri,co))],end=' ')\n",
    "    print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \n",
    "    def __init__(self, environment, gamma, theta):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.policy = CustomPolicy(environment)\n",
    "        # The value of states at step k\n",
    "        self.old_value = dict()\n",
    "        # The value of states at step k+1\n",
    "        self.new_value = dict()\n",
    "        # Initialize the value of each state to 0\n",
    "        for state in environment.get_states():\n",
    "            self.old_value[state] = 0\n",
    "            \n",
    "    def copy_values(self, from_value, to_value):\n",
    "        for state in self.environment.get_states():\n",
    "            to_value[state] = from_value[state]\n",
    "            \n",
    "    def compute_values(self):\n",
    "        while (True):\n",
    "            delta = 0\n",
    "            for state in self.environment.get_states():\n",
    "                # Compute the new value of the state\n",
    "                max_value = -1 * sys.float_info.max\n",
    "                for action in self.environment.get_actions(state):\n",
    "                    value = 0\n",
    "                    for next_state in self.environment.get_states():\n",
    "                        value += self.environment.p(state, action, next_state)*\\\n",
    "                        (self.environment.r(state, action, next_state) + self.gamma * self.old_value[next_state])\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                self.new_value[state] = max_value\n",
    "                delta = max(delta, abs(self.old_value[state] - self.new_value[state]))\n",
    "            # Copy the new value of states into the old ones\n",
    "            self.copy_values(self.new_value, self.old_value)\n",
    "            if delta < self.theta:\n",
    "                return\n",
    "            \n",
    "    def compute_policy(self):\n",
    "        for state in self.environment.get_states():\n",
    "            # Compute the action yielding the maximum return\n",
    "            max_value_of_action = -1 * sys.float_info.max\n",
    "            best_action = None\n",
    "            for action in self.environment.get_actions(state):\n",
    "                value_of_action = 0\n",
    "                for next_state in self.environment.get_states():\n",
    "                    value_of_action += self.environment.p(state, action, next_state) *\\\n",
    "                    (self.environment.r(state, action, next_state) + self.gamma * self.old_value[next_state])\n",
    "                if value_of_action > max_value_of_action:\n",
    "                    best_action = action\n",
    "                    max_value_of_action = value_of_action\n",
    "            self.policy.update(state, best_action)\n",
    "    \n",
    "    def apply(self):\n",
    "        self.compute_values()\n",
    "        self.compute_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "value_iteration = ValueIteration(grid_world, 0.8, 1/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iteration.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT STAY \n"
     ]
    }
   ],
   "source": [
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[value_iteration.policy.apply((ri,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
