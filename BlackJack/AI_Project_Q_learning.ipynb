{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed035a64",
   "metadata": {},
   "source": [
    "# Blackjack game with Q-Learning\n",
    "\n",
    "## Blackjack game\n",
    "\n",
    "The goal of the game is to beat the dealer by having a hand value of 21 or closest to 21 without going over. Each player is dealt two cards, and then has the option to \"hit\" and receive additional cards to improve their hand. They can also choose to “stand” and keep the cards they have. If the player exceeds 21, they “bust” and automatically lose the round. If the player has exactly 21, they automatically win. Otherwise, the player wins if they are closer to 21 than the dealer. \\\n",
    "The value of each card is listed below:\n",
    "- 10/Jack/Queen/King → 10\n",
    "- 2 through 9 → Same value as card\n",
    "- Ace → 1 or 11 (Player’s choice)\n",
    "\n",
    "The game starts with players making their bets, after which the dealer will deal two cards to each player and two to himself, with one card face up and one face down (known as the \"hole\" card). Players then make their decisions to hit or stand. Once all players have completed their turns, the dealer will reveal their hole card and hit or stand according to a set of rules. If the dealer busts (goes over 21), players who have not bust win. If neither the player nor dealer busts, the hand closest to 21 wins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3615039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import enum\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86578d5a",
   "metadata": {},
   "source": [
    "## Setting up the basics of the game\n",
    "\n",
    "### Card and deck\n",
    "\n",
    "The cards' definitions are set up below. \n",
    "- `ranks`: a dictionary that maps the string representations of card ranks to their numerical values\n",
    "- `Suit`: an enumeration of the four suits in a standard deck of playing cards\n",
    "- `Card`: a class that represents an individual card and contains properties such as the card's suit, rank, and value\n",
    "- `Deck`: a class that represents a deck of cards and contains methods for shuffling the deck, dealing cards, peeking at the top card, adding cards to the bottom of the deck, and printing the deck\n",
    "\n",
    "The `Deck` class initializes an empty list of cards when created and populates it with `num` standard decks of playing cards. The `shuffle` method randomly shuffles the cards in the deck. The `deal` method returns the top card of the deck, and the `peek` method returns the top card without removing it from the deck. The add_to_bottom method adds a card to the bottom of the deck, and the `__str__` method returns a string representation of the deck. The `__len__` method returns the number of cards in the deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72c4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = {\n",
    "    \"two\" : 2,\n",
    "    \"three\" : 3,\n",
    "    \"four\" : 4,\n",
    "    \"five\" : 5,\n",
    "    \"six\" : 6,\n",
    "    \"seven\" : 7,\n",
    "    \"eight\" : 8,\n",
    "    \"nine\" : 9,\n",
    "    \"ten\" : 10,\n",
    "    \"jack\" : 10,\n",
    "    \"queen\" : 10,\n",
    "    \"king\" : 10,\n",
    "    \"ace\" : (1, 11)\n",
    "}\n",
    "\n",
    "class Suit(enum.Enum):\n",
    "    spades = \"spades\"\n",
    "    clubs = \"clubs\"\n",
    "    diamonds = \"diamonds\"\n",
    "    hearts = \"hearts\"\n",
    "    \n",
    "class Card:\n",
    "    def __init__(self, suit, number, value):\n",
    "        self.suit = suit\n",
    "        self.number = number\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.number + \" of \" + self.suit.value\n",
    "\n",
    "class Deck:\n",
    "    def __init__(self, num=1):\n",
    "        self.cards = []\n",
    "        for i in range(num):\n",
    "            for suit in Suit:\n",
    "                for number, value in numbers.items():\n",
    "                    self.cards.append(Card(suit, number, value))\n",
    "                \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "        \n",
    "    def deal(self):\n",
    "        return self.cards.pop(0)\n",
    "    \n",
    "    def peek(self):\n",
    "        if len(self.cards) > 0:\n",
    "            return self.cards[0]\n",
    "        \n",
    "    def add_to_bottom(self, card):\n",
    "        self.cards.append(card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        for card in self.cards:\n",
    "            result += str(card) + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad72e0",
   "metadata": {},
   "source": [
    "### Setting up the game's rules\n",
    "\n",
    "The evaluation of the dealer's hand is done here, following a proper set of rules that are predictable. As the game goes, the dealer will be following *Hard 17* rule. This means the dealer will not hit again if the Ace yields a 17. This also means that Aces initially declared as 11's can be changed to 1's as new cards come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6c0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dealer_eval(player_hand):\n",
    "    num_ace = 0\n",
    "    use_one = 0\n",
    "    for card in player_hand:\n",
    "        if card.number == \"ace\":\n",
    "            num_ace += 1\n",
    "            use_one += card.value[0] # It will be using Ace is equal to 1 in here\n",
    "        else:\n",
    "            use_one += card.value\n",
    "    \n",
    "    if num_ace > 0:\n",
    "        ace_counter = 0\n",
    "        while ace_counter < num_ace:\n",
    "            use_eleven = use_one + 10 \n",
    "            \n",
    "            if use_eleven > 21:\n",
    "                return use_one\n",
    "            elif use_eleven >= 17 and use_eleven <= 21:\n",
    "                return use_eleven\n",
    "            else:\n",
    "                use_one = use_eleven\n",
    "            \n",
    "            ace_counter += 1\n",
    "        \n",
    "        return use_one\n",
    "    else:\n",
    "        return use_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a651142",
   "metadata": {},
   "source": [
    "As for the evaluation of the player's hand, it focus on the value of Ace. For instance, if the Ace obtained gets the player to the total sum of 18 to 21, then Ace is equal to 11, otherwise it is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b1ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_eval(player_hand):\n",
    "    num_ace = 0\n",
    "    use_one = 0\n",
    "    for card in player_hand:\n",
    "        if card.number == \"ace\":\n",
    "            num_ace += 1\n",
    "            use_one += card.value[0] # It will be using Ace is equal to 1 in here\n",
    "        else:\n",
    "            use_one += card.value\n",
    "    \n",
    "    if num_ace > 0:\n",
    "        ace_counter = 0\n",
    "        while ace_counter < num_ace:\n",
    "            use_eleven = use_one + 10 \n",
    "            \n",
    "            if use_eleven > 21:\n",
    "                return use_one\n",
    "            elif use_eleven >= 18 and use_eleven <= 21:\n",
    "                return use_eleven\n",
    "            else:\n",
    "                use_one = use_eleven\n",
    "            \n",
    "            ace_counter += 1\n",
    "        \n",
    "        return use_one\n",
    "    else:\n",
    "        return use_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b2b55",
   "metadata": {},
   "source": [
    "Below, the code for the logic in which the dealer will follow is presented. As the action is rather straightforward, nothing much happens. Following the Hard 17 rule, the dealer will stop hitting after the total is 17 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44173a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dealer_turn(dealer_hand, deck):\n",
    "    dealer_value = dealer_eval(dealer_hand)\n",
    "    while dealer_value < 17:\n",
    "        dealer_hand.append(deck.deal()) # Making the Hit\n",
    "        dealer_value = dealer_eval(dealer_hand)\n",
    "\n",
    "    return dealer_value, dealer_hand, deck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591579c",
   "metadata": {},
   "source": [
    "## Setting up the OpenAI environment for Blackjack\n",
    "\n",
    "The solution is to create a custom class in order to train the bot.\n",
    "\n",
    "The class has two main attributes:\n",
    "- the `action_space` attribute, which is a 2-element discrete space representing the two possible actions the player can take: hit (0) or stand (1)\n",
    "- the `observation_space` attribute, which is a tuple representing the state of the game, consisting of two elements: player hand value (18 possible values ranging from 3 to 20) and the dealer's upcard value (10 possible values ranging from 2 to 11).\n",
    "\n",
    "The class also implements four main methods:\n",
    "\n",
    "- `step(action)` method, which takes the player's action as an input, updates the game state, calculates the reward, and returns the new state and the reward.\n",
    "- `reset()` method, which resets the game to its initial state and returns the start state.\n",
    "- `_take_action(action)` method, which updates the player's hand according to the player's action.\n",
    "- `dealer_turn(dealer_hand, bj_deck)` method, which calculates the dealer's final hand value according to the dealer's rules.\n",
    "\n",
    "The game starts with an initial balance of 1000, and a deck of cards made up of 6 decks. The deck is initialized using the `Deck` class. The game can end in three ways: the player stands, the player's hand value exceeds 21, or the dealer's hand value exceeds 21. The rewards are -1 for losing, 0 for tie, and 1 for winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666d55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_BALANCE = 1000\n",
    "NUM_DECKS = 6\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the blackjack deck.\n",
    "        self.bj_deck = Deck(NUM_DECKS)\n",
    "        \n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        self.reward_options = {\"lose\":-1, \"tie\":0, \"win\":1}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.bj_deck.deal())\n",
    "            \n",
    "        # re-calculate the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # End the episode/game is the player stands or has a hand value >= 21.\n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        \n",
    "        # rewards are 0 when the player hits and is still below 21, and they keep playing.\n",
    "        rewards = 0\n",
    "        \n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: # above 21, player loses automatically.\n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                ## Begin dealer turn phase.\n",
    "\n",
    "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
    "\n",
    "                ## End of dealer turn phase\n",
    "\n",
    "                #------------------------------------------------------------#\n",
    "\n",
    "                ## Final Compare\n",
    "\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 3 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "        \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Add the player and dealer cards back into the deck.\n",
    "        self.bj_deck.cards += self.player_hand + self.dealer_hand\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.bj_deck.shuffle()\n",
    "         \n",
    "        self.balance = INITIAL_BALANCE\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 2 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "            \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # convert the player hand into a format that is\n",
    "        # easy to read and understand.\n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.number)\n",
    "            \n",
    "        # re-calculate the value of the dealer upcard.\n",
    "        upcard_value = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7411756",
   "metadata": {},
   "source": [
    "After the environment for Blackjack is created, it is necessary to create the Agent, or bot, that will learn how to play the game.\n",
    "\n",
    "With the learning method of choice defined as the **Q-Learning**, a class `Agent` represents a reinforcement learning agent that can learn from its environment through trial and error. The agent has various attributes such as the environment it interacts with, its exploration factor `epsilon`, learning rate `alpha`, discount factor `gamma`, and the number of episodes it needs to train on `num_episodes_to_train`. The `Agent` class has several methods that allow it to learn from its interactions with the environment:\n",
    "- `train` method: This method updates the exploration factor epsilon and the learning rate alpha after each action the agent takes. The epsilon decreases over time and the agent becomes less explorative as it trains, so that it takes more exploitation-based decisions in the future.\n",
    "\n",
    "- `get_q_value` method: If the agent encounters a new observation, it sets the initial values of the Q-values for each action to 0.0. The Q-values represent the expected long-term reward of taking a specific action in a specific observation.\n",
    "\n",
    "- `get_max_q_value` method: Given an observation, this method returns the maximum Q-value of all the actions that the agent can take.\n",
    "\n",
    "- `choose_action` method: Based on the current observation, the agent uses this method to choose an action to take. If a random number is greater than the exploration factor epsilon, the agent chooses the action with the highest Q-value. Otherwise, it takes a random action.\n",
    "\n",
    "- `learn` method: This method updates the Q-value of the action the agent took based on the reward it received and the utility of the next observation. The new Q-value is computed using a formula that combines the current Q-value with the reward and the discounted utility of the next observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014bc560",
   "metadata": {},
   "source": [
    "Devo modificare questo e il markdown!!!! Spingiamo al top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dcae5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.5, epsilon=0.1, num_episodes=30000):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        #self.decrement = (0.1 * epsilon) / (0.3 * num_episodes)\n",
    "        #self.decrement = (0.99 * epsilon) #/ (0.3 * num_episodes)\n",
    "        self.valid_actions = list(range(self.env.action_space.n))\n",
    "        self.episodes_left = num_episodes\n",
    "        self.Q_table = dict()\n",
    "\n",
    "    #DONE\n",
    "    def get_q_value(self, observation, action):\n",
    "        return self.Q_table.get((observation, action), 0.0)\n",
    "\n",
    "    \n",
    "    #DONE\n",
    "    def choose_action(self, observation):\n",
    "        if (random.random() > self.epsilon):\n",
    "            maxQ = self.get_max_q_value(observation)\n",
    "            if maxQ > -1:\n",
    "                max_q_value = -1\n",
    "                best_action = None\n",
    "                for action in self.valid_actions:\n",
    "                    q_value = self.get_q_value(observation, action)\n",
    "                    if q_value > max_q_value:\n",
    "                        max_q_value = q_value\n",
    "                        best_action = action\n",
    "                        action = best_action\n",
    "                    else:\n",
    "                        action = random.choice(self.valid_actions)\n",
    "            #action = max(self.valid_actions, key=lambda a: self.get_q_value(observation, a)) if maxQ > -1 else random.choice(self.valid_actions)\n",
    "        else:\n",
    "            # explore with epsilon\n",
    "            action = random.choice(self.valid_actions)\n",
    "\n",
    "        #self.train()\n",
    "        return action\n",
    "\n",
    "    #DONE\n",
    "    def learn(self, observation, action, reward, next_observation):\n",
    "        \n",
    "        max_next_value = self.get_max_q_value(next_observation)\n",
    "        temp_diff = self.gamma * max_next_value - self.get_q_value(observation, action)\n",
    "        self.Q_table[(observation, action)] = self.Q_table.get((observation, action), 0.0) + self.alpha * (reward + temp_diff)\n",
    "\n",
    "    #DONE, take the maximum value of Q\n",
    "    def get_max_q_value(self, observation):\n",
    "        max_next_value = -1*np.inf\n",
    "        for action in self.valid_actions:\n",
    "            next_value = self.get_q_value(observation, action)\n",
    "            if next_value > max_next_value:\n",
    "                max_next_value = next_value\n",
    "        return max_next_value\n",
    "\n",
    "    #DONE\n",
    "    def train(self):\n",
    "        if (self.episodes_left > 0):\n",
    "            #self.epsilon -= self.decrement\n",
    "            self.epsilon = self.epsilon*0.99\n",
    "        else:\n",
    "            self.epsilon = 0.0\n",
    "            self.alpha = 0.0\n",
    "\n",
    "        self.episodes_left -= 1\n",
    "        \n",
    "        \n",
    "    def moving_average(self, lst, window = 100):\n",
    "        return [sum(lst[i:i+window])/window for i in range(len(lst)-window+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f673e2",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "This snippet is a simulation of the reinforcement learning agent playing Blackjack. \n",
    "\n",
    "The agent is using a custom environment called `BlackjackEnv` and an `Agent` class, both previously defined here. The agent is trained over a set number of episodes (30000), where it takes actions based on a Q-table and updates the Q-table based on the rewards received from each action. The simulation is run for 1000 rounds with 1000 samples to calculate the average payout per round. The agent's average payout is then plotted and printed, with the running average present for easier comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9596fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the custom model\n",
    "env = BlackjackEnv()\n",
    "\n",
    "agent = QLearningAgent(env=env, epsilon=.9, alpha=0.01, gamma=0.01, num_episodes=30000)\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 1000 # num_rounds simulated over num_samples\n",
    "\n",
    "average_payouts = []\n",
    "win = []\n",
    "win_plot = []\n",
    "c_round = 0\n",
    "\n",
    "observation = env.reset()\n",
    "lista = []\n",
    "\n",
    "for sample in range(num_samples):\n",
    "    round = 1\n",
    "    total_payout = 0 # to store total payout over 'num_rounds'\n",
    "    # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "    while round <= num_rounds:\n",
    "        action = agent.choose_action(tuple(observation))\n",
    "        next_observation, payout, is_done, _ = env.step(action)\n",
    "        if action == 1:\n",
    "            lista.append(\"stick\")\n",
    "        else:\n",
    "            lista.append(\"hit\")\n",
    "        \n",
    "        agent.learn(tuple(observation), action, payout, tuple(next_observation))\n",
    "        total_payout += payout\n",
    "        observation = next_observation\n",
    "        '''\n",
    "        img = env.render()\n",
    "        \n",
    "        if img is not None:\n",
    "            img = img.astype(float)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "        '''\n",
    "        if payout == 1:\n",
    "            win.append(payout)\n",
    "        \n",
    "        if is_done:\n",
    "            observation = env.reset() # Environment deals new cards to player and dealer\n",
    "            lista = []\n",
    "            c_round += 1\n",
    "            win_plot.append(win.count(1)/c_round)\n",
    "            round += 1\n",
    "    agent.train()\n",
    "    average_payouts.append(total_payout)\n",
    "    move_avg = agent.moving_average(average_payouts)\n",
    "    \n",
    "\n",
    "\n",
    "avg_run = average_payouts\n",
    "avg_win = (win.count(1)/(num_rounds*num_samples))*100\n",
    "\n",
    "# Plot payout per 1000 episodes for episode\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(avg_run, 'k-')\n",
    "plt.plot(move_avg, 'r-')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Payout after {} rounds'.format(num_rounds))\n",
    "plt.grid(linestyle=':')\n",
    "plt.show()      \n",
    "    \n",
    "print (\"Average payout after {} rounds is: {}\".format(num_rounds, sum(average_payouts)/(num_samples)))\n",
    "\n",
    "print(\"Win rate: \" + str(avg_win) + \" %\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736d98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
