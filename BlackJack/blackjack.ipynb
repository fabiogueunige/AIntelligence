{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLACKJACK Q-LEARNING\n",
    "\n",
    "Author: Fabio Guelfi  \n",
    "\n",
    "matricola: s5004782\n",
    "\n",
    "### Procedure:\n",
    "I started reading and understanding the [Gymnasium documentation](https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/) about the Blackjack implementation with the Q-Learning reinforcment learning. Starting from that I rebuilt the project using also the method seen during the lesson to develop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mL'esecuzione di celle con 'c:\\msys64\\mingw64\\bin\\python.exe' richiede il pacchetto ipykernel.\n",
      "\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n",
      "\u001b[1;31mComando: 'c:/msys64/mingw64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', natural=False, sab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resetting the environment to get the first obeservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing an Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random action\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# execute the action in the environment and receiving info from that\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Q-Learning agent\n",
    "epsilon-greedy strategy, where we pick a random action with the percentage epsilon and the greedy action (currently valued as the best) 1 - epsilon DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: defaultdict(float))\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "\n",
    "        Args:\n",
    "            obs: The current observation\n",
    "\n",
    "        Returns:\n",
    "            The action to take\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "        \n",
    "    def update(\n",
    "            self,\n",
    "            obs: tuple[int, int, bool],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            terminated: bool,\n",
    "            next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Update the Q-value for the given observation and action pair.\n",
    "\n",
    "        Args:\n",
    "            obs: The current observation\n",
    "            action: The action taken\n",
    "            reward: The reward received\n",
    "            terminated: Whether the episode terminated\n",
    "            next_obs: The next observation\n",
    "        \"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "\n",
    "        self.q_values[obs][action] += self.lr * temporal_difference\n",
    "\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon by multiplying it with epsilon_decay.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like that I should obtain the optimal policy\n",
    "learning_rate = 0.001\n",
    "n_episodes = 1_000_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(agent, usable_ace=False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11),\n",
    "    )\n",
    "\n",
    "    # create the value grid for plotting\n",
    "    value = np.apply_along_axis(\n",
    "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
    "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(\n",
    "        player_count,\n",
    "        dealer_count,\n",
    "        value,\n",
    "        rstride=1,\n",
    "        cstride=1,\n",
    "        cmap=\"viridis\",\n",
    "        edgecolor=\"none\",\n",
    "    )\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"Player sum\")\n",
    "    ax1.set_ylabel(\"Dealer showing\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"Player sum\")\n",
    "    ax2.set_ylabel(\"Dealer showing\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # add a legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# state values & policy with usable ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can olso notice the state values & policy without usable ace (ace counts as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
    "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
