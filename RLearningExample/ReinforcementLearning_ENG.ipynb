{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the environment model (simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract interface for any environment (compare this with the abstract interface for dynamic programming):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Environment is an abstarct base class that inherits from ABC (Abstract Base Class)\n",
    "\n",
    "class BaseEnvironment(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to return the set of states\n",
    "    def get_states(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to return the set of actions available in the state\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_all_actions(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement action execution\n",
    "    def do_action_and_get_reward(self, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>GridWorld</code> environment: a 4x4 grid where the agent can move up-down and left-right (no diagonal moves). Transitions are deterministic. Cell (3,3) is the target and it is the only one with a chance to stop. Rewards are -1 for every move and 10 when the target is reached. The environment is the same we consider to test dynamic programming, but this time the agent does not know about the transition and reward probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnvironment):\n",
    "    \n",
    "    # Top row top column in the grid is (0,0) coordinate\n",
    "    UP = (-1,0)\n",
    "    DOWN = (1,0)\n",
    "    LEFT = (0,-1)\n",
    "    RIGHT = (0,1)\n",
    "    STAY = (0,0)\n",
    "    \n",
    "    # 'dict' mapping actions to symbolic names\n",
    "    label = {(-1,0):\"UP\", (1,0):\"DOWN\", (0,-1):\"LEFT\", (0,1):\"RIGHT\", (0,0):\"STAY\"}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = list()\n",
    "        # States are just a list of coordinates\n",
    "        for ri in range(4):\n",
    "            for co in range(4):\n",
    "                self.states.append((ri,co))\n",
    "        # Actions are a map of state to list of actions to allowable actions in the state\n",
    "        self.actions = dict()\n",
    "        self.actions[(0,0)] = [GridWorld.RIGHT, GridWorld.DOWN]\n",
    "        self.actions[(0,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,3)] = [GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(1,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,3)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,3)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,0)] = [GridWorld.RIGHT, GridWorld.UP]\n",
    "        self.actions[(3,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(3,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(3,3)] = [GridWorld.STAY, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.all_actions = [GridWorld.STAY, GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.current_state = (0,0)\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "        \n",
    "    def get_actions(self, state):\n",
    "        return self.actions[state]\n",
    "    \n",
    "    def get_all_actions(self, state):\n",
    "        return all_actions\n",
    "    \n",
    "    # Deterministc transition function\n",
    "    def transition(self, current_state, action):\n",
    "        # The next state has the following coordinates (x,y)\n",
    "        # x = current_state[0] + action[0]\n",
    "        # y = current_state[1] + action[1] \n",
    "        return (current_state[0] + action[0], current_state[1] + action[1]) \n",
    "    \n",
    "    def do_action_and_get_reward(self, action):\n",
    "        # If the action is not executable in the state, do nothing and return reward 0\n",
    "        if action not in self.get_actions(self.current_state):\n",
    "            return 0\n",
    "        # Compute the next state based on the action\n",
    "        self.current_state = self.transition(self.current_state, action)\n",
    "        if (self.current_state != (3,3)):\n",
    "            # if the new state coincides with the next state the probability is 1\n",
    "            # otherwise the probability is 0\n",
    "            return -1\n",
    "        else:\n",
    "            # ...except the goal \n",
    "            return 10\n",
    "        \n",
    "    def task_done(self):\n",
    "        return self.current_state == (3,3)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = (0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base class for all policies. The method <code>apply</code> enforces the policy, the method <code>update</code> changes the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement policy application\n",
    "    # Returns the action given the state\n",
    "    def apply(self, state):\n",
    "        pass\n",
    "\n",
    "    def update(self, state, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concrete policy to initialize the search for an optimal policy. Implements an $\\epsilon$-greedy technique for action choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, environment, Q_table, epsilon):\n",
    "        self.environment = environment\n",
    "        self.Q_table = Q_table\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, state):\n",
    "        actions = self.environment.get_actions(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            # Choose an action at random with probability epsilon\n",
    "            return random.choice(actions)\n",
    "        else:\n",
    "            # Choose the best action accordin to Q_table with probability 1-epsilon\n",
    "            # If all actions have the same Q-value then break ties randomly\n",
    "            max_action_value = sys.float_info.min\n",
    "            best_action = random.choice(actions)\n",
    "            for action in actions:\n",
    "                if self.Q_table[state][action] > max_action_value:\n",
    "                    max_action_value = self.Q_table[state][action]\n",
    "                    best_action = action\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA algorithm (on-policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \n",
    "    def __init__(self, environment, gamma, alpha, epsilon, episodes):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.Q_table = dict()\n",
    "        # Initialize the value of each state-action pair to 0\n",
    "        for state in environment.get_states():\n",
    "            self.Q_table[state] = dict()\n",
    "            for action in environment.get_actions(state):\n",
    "                self.Q_table[state][action] = 0\n",
    "        # Use epsilon-greedy policy for learning\n",
    "        self.policy = EpsilonGreedyPolicy(environment, self.Q_table, epsilon)\n",
    "            \n",
    "    def apply(self):\n",
    "        for e in range(self.episodes):\n",
    "            state = self.environment.current_state \n",
    "            action = self.policy.apply(self.environment.current_state) \n",
    "            while not self.environment.task_done():\n",
    "                reward = self.environment.do_action_and_get_reward(action)\n",
    "                next_state = self.environment.current_state\n",
    "                next_action = self.policy.apply(next_state)\n",
    "                temp_diff = self.Q_table[next_state][next_action] - self.Q_table[state][action]\n",
    "                self.Q_table[state][action] =\\\n",
    "                self.Q_table[state][action] + self.alpha * (reward + self.gamma * temp_diff) \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            # Must reset the environment before trying another episode\n",
    "            self.environment.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting the SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "sarsa = SARSA(grid_world, 0.9, 0.2, 0.1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the optimal policy $\\pi^*$ based on the $Q^*$ values approximated by SARSA. Creating a new class for a greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, environment, Q_table):\n",
    "        self.environment = environment\n",
    "        self.Q_table = Q_table\n",
    "        \n",
    "    def apply(self, state):\n",
    "        actions = self.environment.get_actions(state)\n",
    "        max_action_value = -1 * sys.float_info.max\n",
    "        best_action = None\n",
    "        for action in actions:\n",
    "            if self.Q_table[state][action] > max_action_value:\n",
    "                max_action_value = self.Q_table[state][action]\n",
    "                best_action = action\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_policy = GreedyPolicy(grid_world, sarsa.Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT RIGHT RIGHT DOWN \n",
      "DOWN RIGHT RIGHT DOWN \n",
      "RIGHT DOWN RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT STAY \n"
     ]
    }
   ],
   "source": [
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[greedy_policy.apply((ri,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the values in the $Q$-table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,0):\n",
      "RIGHT -> -9619.168836722341\n",
      "DOWN -> -9629.453339471214\n",
      "(0,1):\n",
      "RIGHT -> -9588.476628707229\n",
      "LEFT -> -9628.653121787027\n",
      "DOWN -> -9623.179955730731\n",
      "(0,2):\n",
      "RIGHT -> -9511.264371346395\n",
      "LEFT -> -9615.571254825223\n",
      "DOWN -> -9575.05384408508\n",
      "(0,3):\n",
      "LEFT -> -9572.428297889253\n",
      "DOWN -> -9146.73128108183\n",
      "(1,0):\n",
      "RIGHT -> -9616.820355468895\n",
      "DOWN -> -9598.615428556577\n",
      "UP -> -9626.479809595492\n",
      "(1,1):\n",
      "RIGHT -> -9583.8374049192\n",
      "LEFT -> -9628.883395572204\n",
      "DOWN -> -9618.863064087307\n",
      "UP -> -9615.652798438943\n",
      "(1,2):\n",
      "RIGHT -> -9490.539092228908\n",
      "LEFT -> -9616.01339499991\n",
      "DOWN -> -9520.612343120301\n",
      "UP -> -9586.407055723097\n",
      "(1,3):\n",
      "LEFT -> -9553.07674360966\n",
      "DOWN -> -6867.006218288734\n",
      "UP -> -9316.439783456959\n",
      "(2,0):\n",
      "RIGHT -> -9496.93852631532\n",
      "DOWN -> -9504.156402499693\n",
      "UP -> -9626.40275635479\n",
      "(2,1):\n",
      "RIGHT -> -9488.840163294892\n",
      "LEFT -> -9522.659789212608\n",
      "DOWN -> -9468.780487603939\n",
      "UP -> -9624.510760661622\n",
      "(2,2):\n",
      "RIGHT -> -9260.927023339187\n",
      "LEFT -> -9504.247495946158\n",
      "DOWN -> -9406.050696941951\n",
      "UP -> -9585.115962344826\n",
      "(2,3):\n",
      "LEFT -> -9464.535571461685\n",
      "DOWN -> 11.111111111111107\n",
      "UP -> -9522.374972391575\n",
      "(3,0):\n",
      "RIGHT -> -9379.423973307421\n",
      "UP -> -9525.773869694367\n",
      "(3,1):\n",
      "RIGHT -> -7445.05427525656\n",
      "LEFT -> -9464.020074875476\n",
      "UP -> -9491.889701062228\n",
      "(3,2):\n",
      "RIGHT -> 11.111111111111107\n",
      "LEFT -> -9398.38027628239\n",
      "UP -> -9471.292662514781\n",
      "(3,3):\n",
      "STAY -> 0\n",
      "LEFT -> 0\n",
      "UP -> 0\n"
     ]
    }
   ],
   "source": [
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(\"({},{}):\".format(ri,co))\n",
    "        actions = sarsa.environment.get_actions((ri,co))\n",
    "        for action in actions:\n",
    "            print(\"{} -> {}\".format(GridWorld.label[action],sarsa.Q_table[(ri,co)][action]))\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning algorithm (off-policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    \n",
    "    def __init__(self, environment, gamma, alpha, epsilon, episodes):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.Q_table = dict()\n",
    "        # Initialize the value of each state-action pair to 0\n",
    "        for state in environment.get_states():\n",
    "            self.Q_table[state] = dict()\n",
    "            for action in environment.get_actions(state):\n",
    "                self.Q_table[state][action] = 0\n",
    "        # Use epsilon-greedy policy for learning\n",
    "        self.policy = EpsilonGreedyPolicy(environment, self.Q_table, epsilon)\n",
    "            \n",
    "    def apply(self):\n",
    "        for e in range(self.episodes):\n",
    "            while not self.environment.task_done():\n",
    "                state = self.environment.current_state \n",
    "                action = self.policy.apply(self.environment.current_state) \n",
    "                reward = self.environment.do_action_and_get_reward(action)\n",
    "                next_state = self.environment.current_state\n",
    "                # Choose maximum Q-value for next state\n",
    "                max_next_value = -1 * sys.float_info.max\n",
    "                next_actions = self.environment.get_actions(next_state)\n",
    "                for next_action in next_actions:\n",
    "                    next_value = self.Q_table[next_state][next_action]\n",
    "                    if next_value > max_next_value:\n",
    "                        max_next_value = next_value\n",
    "                # Temporal difference is computed differently w.r.t. SARSA\n",
    "                temp_diff = max_next_value - self.Q_table[state][action]\n",
    "                # Update equation is the same as SARSA\n",
    "                self.Q_table[state][action] =\\\n",
    "                self.Q_table[state][action] + self.alpha * (reward + self.gamma * temp_diff) \n",
    "            # Must reset the environment before trying another episode\n",
    "            self.environment.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "q_learning = Qlearning(grid_world, 0.9, 0.2, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT DOWN \n",
      "RIGHT RIGHT RIGHT STAY \n"
     ]
    }
   ],
   "source": [
    "greedy_policy = GreedyPolicy(grid_world, q_learning.Q_table)\n",
    "for ri in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[greedy_policy.apply((ri,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
